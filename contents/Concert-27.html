<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <title></title>
  <meta name="Generator" content="Cocoa HTML Writer">
  <meta name="CocoaVersion" content="1671.6">
  <style type="text/css">
    p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 16.0px Times}
    p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 16.0px Times; min-height: 19.0px}
    span.s1 {font: 16.0px 'Arial Unicode MS'}
    span.Apple-tab-span {white-space:pre}
  </style>
</head>
<body>
<p class="p1"><b>Concert I - Friday Sep. 27</b></p>
<p class="p1"><b>Live Csound</b></p>
<p class="p1"><b>Theatre - 09:15 PM</b></p>
<p class="p2"><b></b><br></p>
<p class="p1"><b>In memory of a brilliant, passionate, and truly gifted young csounder – Shengzheng Zhang (a.k.a. John Towse)</b></p>
<p class="p2"><b></b><br></p>
<p class="p1"><b>First Part</b></p>
<p class="p2"><b></b><br></p>
<p class="p1"><b>Luís Antunes Pena (*)</b></p>
<p class="p1"><b>Im Rauschen, cantabile</b> (2012) - 7'17"<span class="Apple-converted-space"> </span></p>
<p class="p1">For double bass and live electronics<span class="Apple-converted-space"> </span></p>
<p class="p1">Jakob Krupp, double bass</p>
<p class="p2"><br></p>
<p class="p1">“Im Rauschen, cantabile“ is a delirium for double bass and electronics base on the piece „Im Rauschen Rot“ for double bass, percussion quartet and electronics premiered 2010 by Edicson Ruiz and the Drumming - Grupo de Percussão.</p>
<p class="p1">„Im Rauschen, cantabile“ describes a state of constant tension. An organic tension where uncertainty plays an important role. The sounding environment of the piece is constructed like a two voice composition between melodies and sounds of percussive nature. It describes a movement between abstraction and empirical perception.</p>
<p class="p2"><br></p>
<p class="p2"><br></p>
<p class="p1"><b>Richard Boulanger</b></p>
<p class="p1"><b>Cloning A Dinosaur from Trapped DNA</b> (2019) - 12'<span class="Apple-converted-space"> </span></p>
<p class="p1">Live SOLO Modular Synthesizer Performance</p>
<p class="p2"><br></p>
<p class="p1">Controlled, modulated, and spatialized live by an assortment of theremin, joystick, brainwave, and pressure sensors, this 4-channel EuroRack modular synthesizer solo will feature: three Qu-Bit Nebulae modules (each running and rendering, in real-time, voltage-controlled versions of Boulanger’s classic "Trapped in Convert" Csound instruments, and a suite of new custom “Qu-Bit Csound” orchestras); plus two Thonk Music Thing Modular Radio Music modules (each playing a number of pre-rendered custom Csound orchestras as .AIF audio files); plus, two Audio Damage ODIO modules (allowing for the modular system’s audio to be sent to, and returned from two iPads running two Boulanger Labs Csound for iOS apps - csGrain, and csSpectral, that are being used for real-time Phase Vocoding and other Csound-based audio processing).<span class="Apple-converted-space">  </span>This newly composed, and completely re-designed solo version of the work was written to celebrate the 40th anniversary of the world premiere of Boulanger’s classic “Trapped in Convert”. "Back in the summer of 1979, when I composed "Trapped in Convert" at the EMS Studio at MIT using Barry Vercoe’s "music11" language, and then revised it at the MIT Media Lab in 1986 to serve as a beta-test suite for Vercoe’s new "Csound" language, I modeled all my sounds on those I would create in my home studio with my ARP 2600 analog synthesizer. Now, some 40 years later, and to celebrate the 40th anniversary of composition, I have been able to return to the modular synthesizer; to come full circle – but this time, I am making Csounds on the modular thanks to the design and commercial availability of the amazing Nebulae module from Qu-Bit Electronics, (the Nebulae is a EuroRack synthesizer module built around a Raspberry Pi running the latest version of Csound under the hood!). And so, as you might well imagine, I am incredibly excited by the fact that this old “Dinosaur” is no longer “Trapped” and I look look forward to letting her loose at the 5th International Csound Conference, here in Italy, and taking her out for a run around the hall!"</p>
<p class="p2"><br></p>
<p class="p2"><br></p>
<p class="p1"><b>Guenter Steinke</b></p>
<p class="p1"><b>Arcade </b>(1991/2019) - 11'<span class="Apple-converted-space"> </span></p>
<p class="p1">For cello and live electronics<span class="Apple-converted-space"> </span></p>
<p class="p1">Csound version by Joachim Heintz, with Marijana Janevska, Daria Cheikh-Sarraf, Farhad Illaghi Hosseini, Philipp Henkel, Shadi Kassaee, Dilxat Dawut, Hunjoo Jung<span class="Apple-converted-space"> </span></p>
<p class="p1">Nigel Thean, cello</p>
<p class="p2"><br></p>
<p class="p1">"Arcade" has been written in the "Experimentalstudio des Südwestfunks” (in which Luigi Nono worked in the 1980s) in close collaboration with Lucas Fels (violoncello).<span class="Apple-converted-space"> </span></p>
<p class="p1">It used delays, harmonizers, filters, reverb and a spatialization tool ("Halafon"). In 2002 it has been ported to MaxMSP and in 2010 to PD.<span class="Apple-converted-space"> </span></p>
<p class="p1">The Csound version was first performed in Sprengel Museum Hannover in april 2019 with Ulrike Brand, violoncello, and under guidance of Steinke who authorized this version as most authentic.</p>
<p class="p2"><br></p>
<p class="p2"><br></p>
<p class="p1"><b>Tarmo Johannes</b></p>
<p class="p1"><b>Dark-warm</b> (2019) - 8'/13'<span class="Apple-converted-space"> </span></p>
<p class="p1">For DIY contrabass flute, live electronics and audience on smartphones</p>
<p class="p1">Tarmo Johannes, bass flute</p>
<p class="p2"><br></p>
<p class="p1">The whole realization is written in Csound, played from CsoundQt.<span class="Apple-converted-space">  </span>The general technique is inspired by<span class="Apple-converted-space">  </span>"Calls from the Fog" by Enda Bates for flute and electronics. The live sound is band-pass filtered into separate buffers that will be played back and manipulated according to certain rules. No pre-recorded sound is used.<span class="Apple-converted-space">  </span>In one part of the piece the spectrum of the sound is analyzed and "made audible" by playing the most prominent harmonics as separate sounds. The web based instrument of participants<span class="Apple-converted-space">  </span>is written using WebAudio Csound and uses a version of Hans Mikelson's wage-guide flute.<span class="Apple-converted-space"> </span></p>
<p class="p2"><br></p>
<p class="p1"><b>* Call for Music, selected Composer</b><span class="s1"><br>
</span></p>
<p class="p1"><b>Second Part</b></p>
<p class="p2"><b></b><br></p>
<p class="p1"><b>Øyvind Brandtsegg</b></p>
<p class="p1"><b>Superstring Theories</b> (2018) - 10'<span class="Apple-converted-space"> </span></p>
<p class="p1">For solo instrument and live electronics</p>
<p class="p1">Bernt Isak Wærstad (co-musician), physical model of a string</p>
<p class="p1">over the internet, with one end in Cagli and one end in Oslo</p>
<p class="p2"><br></p>
<p class="p1">Superstring Theories is based on a physical model of a string, divided between the two musicians. It is a one-string two-player instrument. Each of them has part of the string, can tune it and inject energy into it by means of contact microphones, while the injected energy also resonates in the other musician's part of the string. When one part is tunes, this also affects the tuning of the other part. Each part has it's own resonance circuit, as well as the global one spanning the whole string. This means that some local control over pitch is maintained even when the other part of the string is retuned. For some performances of the piece, the musicians are geographically separated locations, connecting the pieces of the string together over remote network connections, In other performances, the two musicians are located at separate ends of the same room.<span class="Apple-converted-space"> </span></p>
<p class="p2"><br></p>
<p class="p2"><br></p>
<p class="p1"><b>Richard Boulanger / Steven YI</b></p>
<p class="p1"><b>Integrating Thought and Time Patterns – Reflecting on Captured Moments</b> (2019) - 15'<span class="Apple-tab-span">	</span></p>
<p class="p1">A duet by Steven Yi, live coding and Richard Boulanger, live sampling</p>
<p class="p2"><br></p>
<p class="p1">Integrating Thought and Time Paterns – Reflecting on Captured Moments (2019) brings together the live coding work of Steven Yi, with the live sampling work of Richard Boulanger.<span class="Apple-converted-space">  </span>Using his extensive instrument library from live.csound.com and the live coding language and system that he has developed, refined, and perfected over the past few years, Steven will generate and transform melodic and rhythmic patterns that will be fed into Boulanger’s modular system where they will be sampled (using the Qu-bit Nebulae2), and “frozen”, “time-scaled”, “granularized” “reversed”, “pitch-shifted” (using the Nebulae module and the csSpectral and csGrain iPad apps),<span class="Apple-converted-space">  </span>Yi’s patterns will trigger and control Karplus-Strong plucked strings, modal resonators and vocal synthesizers (via the 2hp Pluck, Ring, and Vowel modules) and excite and modulate Scanned synthesis resonators (via the Qu-Bit Scanned modules).<span class="Apple-converted-space">  </span>And everything will be spatialized, filtered, reverberated, and equalized under the brainwave and biosensor control signals measured from Boulanger head, hands, and heart (using the Instruō Scion and soundmachines Bl1 brainterface modules).<span class="Apple-converted-space">  </span>This world premiere literally sonifies Yi and Boulanger’s “Mind Models” as they integrate them over space and time.</p>
<p class="p2"><br></p>
<p class="p2"><br></p>
<p class="p1"><b>Alex Hofmann</b></p>
<p class="p1"><b>65.4</b> (2019) - 3'30"<span class="Apple-converted-space"> </span></p>
<p class="p1">Live electronics including digital processing, embedded systems, live coding<span class="Apple-converted-space"> </span></p>
<p class="p1">Alex Hofmann, halfphysler<span class="Apple-converted-space"> </span></p>
<p class="p1">Sebastian Schmutzhard, cello and COSMO live electronics<span class="Apple-converted-space"> </span></p>
<p class="p2"><br></p>
<p class="p1">65.4 is a drone piece for live-processed Cello and Halfphysler. Inspired<span class="Apple-converted-space"> </span></p>
<p class="p1">by the opening of the music podcast 'Klangangriff' by the Georgian<span class="Apple-converted-space"> </span></p>
<p class="p1">artist Yakkushi, Hofmann and Schmutzhard teleport the sound atmosphere onto a live stage using the cello and physical modelling with Csound. The live processing of the Cello is based on effects of the COSMO library and runs on the embedded ultra low-latency platform Bela.<span class="Apple-converted-space"> </span></p>
<p class="p1">The Halfphysler is a hybrid acoustic-virtual single reed instrument that has been presented at NIME 2019 the first time. This novel instrument is comprised of a sensor equipped clarinet reed and a virtual tube,<span class="Apple-converted-space">  </span>simulated in realtime using Csound and the Bela board.</p>
<p class="p2"><br></p>
<p class="p2"><br></p>
<p class="p1"><b>Shadi Kassaee (*)</b></p>
<p class="p1"><b>Duo for Half-Physler and Alma</b> (2019) - 3'59"<span class="Apple-converted-space"> </span></p>
<p class="p1">Live electronics</p>
<p class="p2"><br></p>
<p class="p1">The piece is an improvisational duo between the Half-Physler model and Alma. The Half-Physler is a prototype of a hybrid singe-reed instrument. The excitation mechanism of the instrument is based on actual instrument parts (mouth-piece, saxophone reed, ligature), but the tube is simulated on the Bela board. A bending sensor is directly attached to the reed which allows a large variety of possibilities for stimulating the reed, for example rubbing the reed against different materials and surfaces like foam, rubber, wood, metal, which produce a rich pallet of different and complex timbres, while the simulated tube is controlled by sliders. Alma is an instrument for improvising and composing. It receives the audio input from the Half-Physler, which is analyzed for sounding units of four different sizes, that are played back in different ways by a player using a MIDI keyboard.</p>
<p class="p2"><br></p>
<p class="p1"><span class="Apple-converted-space"> </span><b>* Call for Music, selected Composer</b></p>
</body>
</html>
